# -*- coding: utf-8 -*-
"""embedder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E34N7Z6LCktMtDt7VGURV8sM4PEetOx9
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install gensim scikit-learn

import json
import gensim.downloader as api
from sklearn.decomposition import PCA
import numpy as np
import sys

# ניתן להעביר מספר מילים כארגומנט, ברירת מחדל 5000
limit = 5000
#  מודל GloVe עם 100 ממדים (מספיק עשיר למחקר אך קל משקל)
gensim_model_name = "glove-wiki-gigaword-100"
print(f"--- Loading GloVe model (this may take a minute on first run)... ---")
model = api.load(gensim_model_name)

# ניקח רק את X המילים הנפוצות ביותר כדי לשמור על יעילות
words = model.index_to_key[:limit]
full_vectors = [model[word].tolist() for word in words]

print(f"--- Performing PCA (300D/100D -> 50D)... ---")
# ביצוע PCA ל-50 רכיבים
pca = PCA(n_components=50)
pca_result = pca.fit_with_rotations(np.array(full_vectors)) if hasattr(pca, 'fit_with_rotations') else pca.fit_transform(np.array(full_vectors))

# הכנת מבני הנתונים ל-JSON
full_space_data = []
pca_space_data = []

for i, word in enumerate(words):
    full_space_data.append({
        "word": word,
        "vector": full_vectors[i]
    })
    pca_space_data.append({
        "word": word,
        "vector": pca_result[i].tolist()
    })

print(f"--- Saving files... ---")

with open('full_vectors.json', 'w', encoding='utf-8') as f:
    json.dump(full_space_data, f, ensure_ascii=False)

with open('pca_vectors.json', 'w', encoding='utf-8') as f:
    json.dump(pca_space_data, f, ensure_ascii=False)

print(f"--- Success! Created 'full_vectors.json' and 'pca_vectors.json' ---")
print(f"--- Number of words: {len(words)} ---")